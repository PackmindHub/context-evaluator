# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

# Development Workflow

## After Every Code Modification

**MANDATORY**: Run tests after each code change to catch regressions early:
```bash
bun run test
```

**If any tests fail, you MUST fix them before completing the task.** Do not proceed to lint or consider the task done until all tests pass.

At the end of any task, also run lint and fix issues:
```bash
bun run lint
```

## Pre-commit Hook

This project uses Husky with a pre-commit hook that runs:
- `bun run test` - All tests must pass
- `bun run lint` - Code must pass linting
- `bun run typecheck` - TypeScript types must be valid

Commits will be blocked if any fails.

---

# Common Commands

```bash
# Development
bun run dev              # Both API (port 3001) and frontend (port 3000)
bun run dev:api          # API server only
bun run dev:frontend     # Frontend dev server only
bun run dev:cli          # CLI mode with Claude

# Testing & Quality
bun run test             # Run all tests
bun run lint             # Biome check (lint + format)
bun run lint:fix         # Biome auto-fix
bun run format           # Biome format only
bun run typecheck        # TypeScript check

# Production Build
bun run build            # Full production build (frontend + embedded assets + all platform binaries)

# ⚠️ IMPORTANT: Do NOT use build:binaries directly
# build:binaries only compiles binaries without rebuilding frontend
# Always use the full build command above

# Evaluation
bun run evaluate         # Run CLI evaluation shorthand

# Frontend CSS/Style Changes
cd frontend && bun run css:build  # Rebuild Tailwind CSS (required after Tailwind config changes)
```

## Frontend Style Changes

**IMPORTANT**: When modifying Tailwind configuration (`tailwind.config.js`) or adding new Tailwind utility classes, you **must rebuild the CSS** for changes to appear:

```bash
cd frontend
bun run css:build    # Rebuild CSS once
# OR
bun run css:watch    # Auto-rebuild on changes
```

**Why this is needed**: Tailwind generates utility classes at build time. Changes to `tailwind.config.js` (like adding custom colors) won't appear in the browser until the CSS is regenerated.

**When to rebuild**:
- After modifying `frontend/tailwind.config.js`
- When new Tailwind utility classes aren't showing up
- When color/theme changes don't appear in the browser

## CSS Build Architecture

The application uses a **unified Tailwind CSS build** across all serving modes:

### Development Mode

- **Source**: `frontend/src/styles.css` (Tailwind directives + custom CSS)
- **Output**: `frontend/dist/output.css` (85KB unminified)
- **Generated by**: `tailwindcss -i ./src/styles.css -o ./dist/output.css`
- **Referenced in**: `frontend/index.html` via `<link rel="stylesheet" href="./dist/output.css">`
- **Served from**: Frontend dev server at `localhost:3000`

### Production Mode

- **Source**: `frontend/src/styles.css` (same as dev)
- **Output**: `frontend/dist/styles.css` (58KB minified)
- **Generated by**: `tailwindcss -i ./src/styles.css -o ./dist/styles.css --minify`
- **Referenced in**: `frontend/dist/index.html` via `<link rel="stylesheet" href="./styles.css">`
- **Embedded in**: `src/embedded/frontend-assets.ts` for binary distribution

### Important: Do NOT Import CSS in React Components

- CSS is loaded via HTML `<link>` tags, NOT via `import './styles.css'` in components
- This prevents Bun's build system from extracting CSS into chunk files (e.g., `chunk-abc123.css`)
- Keeps CSS generation under explicit Tailwind CLI control
- Ensures consistency between dev and production CSS

### Production Build Pipeline: CSS Generation

The `build:prod` pipeline in `frontend/package.json` handles CSS with these steps:

1. **`build:prod:js`** - Bundles JavaScript with `--no-bundle-css` flag
   - Creates `App.[hash].js` only, NO chunk CSS files
   - This flag is critical to prevent Bun from extracting CSS

2. **`build:prod:css`** - Generates complete Tailwind CSS
   - Creates `styles.css` with all required utilities
   - Minified and ready for production

3. **`build:prod:html`** - Rewrites HTML references
   - Removes old dev CSS references
   - Adds reference to `styles.css`
   - Inserts bundled JavaScript filename

4. **`build:prod:verify`** - Validates the build
   - Ensures HTML references `styles.css`
   - Ensures no `chunk-*.css` files are referenced
   - Ensures no dev `output.css` remains

## Production Build Pipeline

**CRITICAL**: Always use `bun run build` for production builds. Never use `build:binaries` directly.

### Build Pipeline Stages

The full build process runs these steps in order:

```
clean → generate:version → build:frontend → generate:embedded → build:binaries
```

1. **`clean`** ⚠️ **IMPORTANT**
   - Removes `dist/` (backend build output and binaries)
   - Removes `frontend/dist/` (frontend build output)
   - Removes generated embedded assets (`frontend-assets.ts`, `prompts-assets.ts`)
   - **Ensures a completely fresh build with no stale files**

2. **`build:frontend`**
   - Wipes and recreates `frontend/dist/` directory
   - Compiles React app with hashed filenames (e.g., `App.[hash].js`)
   - Builds minified Tailwind CSS as `styles.css`
   - Copies public assets

3. **`generate:embedded`**
   - Reads `frontend/dist/` directory
   - Embeds all assets as strings into `src/embedded/frontend-assets.ts`
   - Creates importable TypeScript modules with base64-encoded binaries

4. **`build:binaries`**
   - Compiles `src/index.ts` (which imports embedded assets)
   - Creates standalone executables for all platforms
   - Output: `dist/bin/context-evaluator-{platform}`

### Why You MUST Clean Before Building

**Problem**: Development mode (`bun run dev`) and production mode use different files:
- **Dev mode** serves `frontend/dist/output.css` (development CSS)
- **Production mode** embeds `frontend/dist/styles.css` (minified CSS)
- Stale files from previous builds can cause UI inconsistencies

**Solution**: The `clean` script ensures:
- No mix of dev and production files
- Frontend is completely rebuilt from scratch
- Embedded assets reflect the actual production build
- Binaries contain the correct UI code

### Why build:binaries Alone Is Broken

If you run `bun run build:binaries` directly:
- ❌ Frontend is **not** rebuilt
- ❌ Old dev files may still exist in `frontend/dist/`
- ❌ Embedded assets are **not** regenerated
- ❌ Binaries contain **stale/mixed** UI code
- ❌ UI will look **different** between dev and production

**The binaries will appear to build successfully, but will serve incorrect frontend code.**

### Correct Build Workflow

**Always do a full clean build:**
```bash
bun run build
```

**Never skip the clean step** - it prevents dev/production UI mismatches.

**After the build:**
- Test the binary: `./dist/bin/context-evaluator-darwin-arm64 api`
- Verify UI matches development mode
- Check that all CSS and assets load correctly

**Development iteration** (no binary needed):
```bash
bun run dev              # Hot reload for frontend + backend
```

**If you suspect stale files:**
```bash
bun run clean && bun run build
```

---

# Architecture Overview

## Dual Execution Modes

The application runs in two modes from a single entry point (`src/index.ts`):
- **CLI Mode**: Direct terminal evaluation via Commander.js
- **API Mode**: Web server with REST API + React frontend

## Directory Structure

```
src/
├── index.ts              # Entry point (routes to CLI or API)
├── cli/                  # CLI-specific code
│   ├── commands/         # Command handlers (evaluate-command.ts)
│   └── output/           # Report formatters (terminal, JSON, raw)
├── api/                  # Web API server
│   ├── routes/           # REST endpoints
│   ├── jobs/             # Job queue management
│   ├── db/               # In-memory evaluation repository
│   └── sse/              # Server-Sent Events for progress
├── shared/               # Core evaluation engine (used by both modes)
│   ├── evaluation/       # Core logic (engine, runner, config)
│   ├── providers/        # AI provider abstraction (claude, opencode)
│   ├── claude/           # Prompt building and response parsing
│   ├── file-system/      # File discovery and git operations
│   └── types/            # Shared TypeScript types
├── web/                  # React frontend (browser build)
└── embedded/             # Single binary asset bundling

frontend/                 # Separate frontend dev build
prompts/evaluators/       # 18 markdown evaluator prompts
```

## TypeScript Path Aliases

```
@shared/* → ./src/shared/*
@cli/*    → ./src/cli/*
@api/*    → ./src/api/*
@web/*    → ./src/web/*
```

## AI Providers

This tool supports multiple AI providers for evaluation:

### Claude Code (default)
- **CLI**: `claude`
- **Usage**: `--agent claude` (or omit for default)
- **Setup**: Install from https://claude.ai/code

### OpenCode
- **CLI**: `opencode`
- **Usage**: `--agent opencode`
- **Model**: openai/gpt-5.2-codex

### Cursor Agent
- **CLI**: `agent`
- **Usage**: `--agent cursor`
- **Setup**: Install Cursor CLI from https://cursor.com
- **Requirements**: `agent` command must be in PATH
- **Verify**: `agent --version`

### OpenAI Codex
- **CLI**: `codex`
- **Usage**: `--agent codex`
- **Setup**: Install from https://docs.openai.com/codex
- **Requirements**: `codex` command must be in PATH
- **Authentication**: Set `CODEX_API_KEY` environment variable or run `codex login`
- **Verify**: `codex --version`

Check available providers:
```bash
bun run src/index.ts cli evaluate --help
```

---

# Evaluation Engine

## Core Flow

1. Determine working directory (clone repo, local path, or current)
2. Find AGENTS.md and CLAUDE.md files (treated equivalently)
3. Identify project context (languages, frameworks, patterns)
4. Choose evaluation mode (unified vs independent based on token count)
5. Run all evaluators with file filtering
6. Parse results, categorize issues (errors vs suggestions)
7. Optional curation to top N impactful issues

## Evaluator Configuration

Evaluators are defined in `src/shared/evaluation/evaluator-types.ts`:
- **Errors** (issues to fix): evaluators 01-10, 13, 17, 19
- **Suggestions** (improvements): evaluators 11, 12, 14, 15
- `executeIfNoFile` determines if evaluator runs without AGENTS.md
- **Note**: Evaluator 16 was merged into 03 (command-completeness). Evaluator 18 was removed.

## Evaluator Coverage Areas

The 17 evaluators are specialized to avoid overlap. Cross-references between evaluators prevent duplicate reporting.

**Content Focus & Command Quality (Clear Boundary):**
- **01-content-quality** (ERROR): Detects human-focused, README-style, and completely irrelevant content
  - Catches: Marketing language, social content, historical narratives, completely off-topic material (recipes, personal stories)
  - Sections: Human-focused content (1.1), vague instructions (1.2), README-style content (1.3), missing critical dev info (1.4), future plans (1.5), skills extraction (1.6)
  - Boundary: Evaluates content FOCUS and RELEVANCE for AI agents
- **03-command-completeness** (ERROR): Evaluates quality of documented commands (assumes commands exist and content is technical)
  - Catches: Unclear commands, missing prerequisites, incomplete setup steps, undocumented env vars, version constraints
  - Sections: Non-executable commands (3.1), missing success criteria (3.2), missing context (3.3), dependency order (3.4), env vars (3.5), versions (3.6)
  - Boundary: Evaluates command QUALITY and COMPLETENESS, not content relevance
  - **Critical Exclusion**: Does NOT flag completely irrelevant content with zero commands → defers to Evaluator 01

**Boundary Rule**: If content has zero commands AND is clearly not technical (recipes, social content) → Evaluator 01. If content has commands but they're unclear/incomplete → Evaluator 03. If technical content lacks commands that should exist → Evaluator 01 (Section 1.4).

**Command & Environment (Merged):**
- **03-command-completeness** (ERROR): Unified evaluator for all command documentation
  - Sections: Command clarity (3.1-3.3), dependencies (3.4), env vars (3.5), versions (3.6)
  - Previously split between 03 and 16, now consolidated

**Code Style Evaluators:**
- **05-code-style** (ERROR): Evaluates quality of documented code style in AGENTS.md
- **12-context-gaps** (SUGGESTION, section 12.1): Scans codebase for undocumented code conventions
  - Complementary: 05 checks documented quality, 12.1 discovers undocumented patterns

**Project Structure Evaluators:**
- **08-project-structure** (ERROR): Quality of documented structure/organization
- **11-subdirectory-coverage** (SUGGESTION): Identifies packages needing their own AGENTS.md
  - Cross-reference: 08 checks docs quality, 11 suggests file creation

**Workflow Evaluators:**
- **07-workflow-integration** (ERROR): Quality of documented git/CI workflows
  - **Cross-reference to 03**: Defers undefined workflow commands to Command Completeness evaluator
  - Focus: Workflow CONTEXT (git conventions, CI/CD, branching), not command definitions

**Language & Clarity:**
- **06-language-clarity** (ERROR): Ambiguous references, vague language, excessive jargon
  - **Cross-reference to 03**: Defers command/tool names to Command Completeness evaluator
  - Focus: Domain terminology and project concepts, not executable commands
  - ✅ Flags: Domain jargon (acronyms, business terms), ❌ Doesn't flag: Command names

**Security vs Configuration:**
- **09-security** (ERROR): Exposed credentials, security risks
  - **Excludes:** Missing env var documentation → see 03-command-completeness (section 3.5)

**Completeness & Balance:**
- **10-completeness** (ERROR): Content balance — too short, too verbose, misplaced, or bloated
  - Sections: Skeletal content (10.1), over-detailed/tutorial content (10.2), inappropriate placement (10.3), LLM-generated verbal bloat (10.4)
  - 10.4 detects phrase-level padding: boilerplate preambles, verbose wrappers, unnecessary justifications, obvious-for-AI definitions, intra-file repetition
  - **Boundary with 10.2**: 10.2 = structural over-documentation (500-line tutorials); 10.4 = phrase-level noise in otherwise well-structured content
  - **Boundary with 01.2**: 01.2 = vague/non-actionable; 10.4 = verbose-but-clear instructions that could be 80% shorter

**Codebase-Scanning Evaluators (Coordinated):**
- **12-context-gaps**: Framework patterns, architecture, tools, domain conventions
  - **Excludes:** Testing (→14) and Database (→15)
- **14-test-patterns-coverage**: All testing patterns (frameworks, mocking, fixtures, E2E)
- **15-database-patterns-coverage**: All database patterns (ORM, migrations, relationships)
- **19-outdated-documentation**: Verifies documented paths/commands exist in codebase

These four evaluators coordinate scanning to avoid duplicate file traversal.

**Database/ORM/Framework Verification:**
- **19-outdated-documentation** (ERROR, section 19.6): Verifies documented technologies match actual usage
  - Checks: Database claims (MongoDB vs PostgreSQL), ORM claims (Mongoose vs TypeORM), state management, major libraries
  - Uses: Import scanning, package.json analysis, Docker Compose inspection, schema file detection
  - Cross-reference: 12.1 discovers undocumented frameworks, 19.6 verifies documented claims are accurate
  - Example: "Docs say MongoDB but codebase uses PostgreSQL" → 19.6 flags this (severity 9-10)

**Irrelevant Technology Guidelines:**
- **19-outdated-documentation** (ERROR, section 19.7): Detects guidelines written for wrong technology stack
  - Catches: Java/Spring guidelines in Node.js project, Python patterns in Go codebase, Angular decorators in React project
  - Detection: Scans for technology-specific patterns (@Controller, mvn, pip, go build) and compares against actual project tech
  - Threshold: Requires 3+ distinct technology patterns AND strong evidence of actual project tech (file counts, build configs)
  - Cross-reference: 01 catches completely off-topic content, 19.6 verifies explicit claims, 19.7 catches wrong-tech guidelines
  - Example: "Java @Controller patterns in Node.js/TypeScript project" → 19.7 flags this (severity 9-10)
  - **Edge cases**: Does NOT flag monorepos, migration docs, or polyglot projects with multiple legitimate technologies

## File Filtering Strategies

In `src/shared/evaluation/config.ts`:
- `ALL_FILES`: Include all discovered files (default)
- `ROOT_ONLY`: Only use shallowest context file
- `CUSTOM`: Custom filter function

## File Path Resolution for Multiple AGENTS.md Files

When a repository contains multiple AGENTS.md or CLAUDE.md files (e.g., root + subdirectories), evaluators may reference files using:
- **Full relative path** (preferred): `packages/AGENTS.md`
- **Basename only** (legacy): `AGENTS.md`

**Resolution Convention:**
- When evaluators use basename-only references (e.g., `"file": "AGENTS.md"`), the system assumes they mean the **root-level file** (`./AGENTS.md`)
- If no root-level file exists but multiple subdirectory files match, snippet extraction will fail with an ambiguity error
- This convention avoids forcing evaluators to use full paths while maintaining clarity

**Implementation:**
- Logic in `src/shared/claude/response-parser.ts` (populateSnippets function)
- Warning displayed in `src/shared/evaluation/runner.ts` indicates which file is the default

## Configuration Options Flow

**CRITICAL**: When adding new configuration options in the frontend, you MUST ensure they flow through the entire stack.

### Required Flow Path

Frontend → API → Engine → Runner Functions

**Each layer must explicitly pass the option to the next layer.**

### Implementation Checklist

When adding a new evaluation configuration option:

1. **Frontend Layer** (`frontend/src/hooks/useEvaluationApi.ts`):
   - Add the parameter to `submitJob()` function signature
   - Include it in the `options` object sent to `/api/evaluate`

2. **API Layer** (`src/api/routes/evaluation.ts`):
   - Options are automatically passed through in `body.options`
   - No changes typically needed (options object is forwarded)

3. **Engine Layer** (`src/shared/evaluation/engine.ts`):
   - **CRITICAL**: Pass the option to runner functions:
     - `runUnifiedEvaluation()` call (line ~496)
     - `runAllEvaluators()` call (line ~834)
   - Add the option to the function call's options object

4. **Runner Layer** (`src/shared/evaluation/runner.ts`):
   - Accept the option in function parameters
   - Use it to control evaluation behavior

### Example: evaluatorFilter Option

```typescript
// Frontend: useEvaluationApi.ts
submitJob(repoUrl, evaluators, provider, evaluatorFilter)

// API: evaluation.ts (automatic passthrough)
body.options.evaluatorFilter

// Engine: engine.ts
const unifiedResult = await runUnifiedEvaluation(agentsFiles, getRelPath, {
  // ... other options
  evaluatorFilter: options.evaluatorFilter, // ← MUST PASS EXPLICITLY
});

const evaluations = await runAllEvaluators(filePath, {
  // ... other options
  evaluatorFilter: options.evaluatorFilter, // ← MUST PASS EXPLICITLY
});

// Runner: runner.ts
export async function runAllEvaluators(
  agentsFilePath: string,
  options: {
    evaluatorFilter?: EvaluatorFilter, // ← MUST ACCEPT
    // ... other options
  }
)
```

### Common Mistake

**DO NOT** assume options automatically propagate. Each function call must explicitly include the option in its parameters.

**Bug Example**: The `evaluatorFilter` option was being sent from the frontend and accepted by the API, but the engine wasn't passing it to the runner functions. This caused all 19 evaluators to run instead of respecting the filter selection.

**Fix**: Added explicit `evaluatorFilter: options.evaluatorFilter` to both runner function calls in `engine.ts`.

### Testing

After adding a new configuration option:
1. Add unit tests in the appropriate test files
2. Verify the option is used at each layer
3. Test that changing the option produces different behavior
4. Add regression tests to prevent future breakage

---

# Temporary Data Cleanup

## Automatic Cleanup After Successful Evaluation

After successful evaluation completion, temporary data is automatically removed:
- **Debug output files** (`debug-output/` directory with prompts and responses)
- **Empty temporary clone directories** (`tmp/clones/` at project root)

**Important:** Database records in `data/evaluations.db` are NOT automatically cleaned up.

## Preserving Debug Output

To keep debug output for manual inspection:
- **CLI**: Use `--preserve-debug-output` flag
- **API**: Set `options.preserveDebugOutput: true` in request body

Debug files are always preserved on evaluation failure for troubleshooting.

## Implementation Details

The cleanup functionality is implemented in `src/shared/evaluation/cleanup-manager.ts` and integrated into the evaluation engine at `src/shared/evaluation/engine.ts`.

Cleanup occurs in the `finally` block of the evaluation engine, ensuring it runs regardless of success or failure (but only removes files on success).

**Cleanup behavior:**
- On success with `preserveDebugOutput=false`: Cleans debug output and empty directories
- On success with `preserveDebugOutput=true`: Only cleans empty directories
- On failure: Preserves all debug data for troubleshooting
- All cleanup errors are logged but do not affect evaluation results

---

# Runtime Prompt Debugging

## Always-On Persistent Logging

The evaluation system automatically logs all runtime evaluator prompts to `prompts/debug/` at the project root for real-time debugging purposes. This is a **separate system** from the temporary debug output controlled by the `--debug` flag.

**Key Characteristics:**
- **Always enabled**: Runs on every evaluation without configuration
- **Persistent storage**: Files accumulate over time and are NOT automatically cleaned up
- **Write-before-execute**: Prompts are written to disk before the AI provider is invoked, ensuring they're preserved even if evaluation crashes
- **Project root location**: Always saved to `{project-root}/prompts/debug/`, even when evaluating cloned repositories

## Filename Format

Prompts are saved with timestamp-based filenames to track execution history:

```
{evaluator-name}[-unified]-{ISO-timestamp}.md
```

**Examples:**
- Independent mode: `content-quality-2026-01-26T15-30-45.md`
- Unified mode: `command-completeness-unified-2026-01-26T15-31-02.md`

## Use Cases

- **Real-time inspection**: View exact prompts sent to AI providers during evaluation
- **Prompt engineering**: Analyze and improve evaluator prompts based on actual runtime content
- **Debugging failures**: Prompts are preserved even if evaluation crashes or times out
- **Historical tracking**: Compare prompts across multiple evaluation runs

## Manual Cleanup

Files in `prompts/debug/` are never automatically deleted. To clean up:

```bash
rm -rf prompts/debug/
```

Or selectively remove old files:

```bash
# Remove files older than 7 days
find prompts/debug/ -name "*.md" -mtime +7 -delete
```

## Implementation

The logging functionality is implemented in `src/shared/evaluation/runtime-prompt-logger.ts` and integrated into both independent and unified evaluation modes in `src/shared/evaluation/runner.ts`.

**Error handling:** All logging errors are caught and logged as warnings. Failed logging never interrupts evaluation execution.

---

# API Endpoints

```
POST /api/evaluate              # Start evaluation job
GET  /api/evaluate/:jobId       # Get job status
GET  /api/evaluate/:jobId/progress  # SSE stream for progress
GET  /api/evaluators            # List available evaluators
GET  /api/health                # Health check
```

## Job Status Flow

`queued` → `running` → `completed` (or `failed`)

Real-time progress via Server-Sent Events (7 event types).

---

# Deployment

## Remote API Server Access

By default, the API server binds to `localhost` for security. This prevents external connections and keeps the server accessible only from the local machine. To accept remote connections on a server or network-accessible host, you must explicitly configure the bind address.

### Option 1: Using CLI Flag (Recommended)

The `--host` and `--port` flags provide explicit control over the server's network binding:

```bash
# Bind to all interfaces (accepts connections from any IP)
./context-evaluator-linux-x64 api --host 0.0.0.0 --port 3001

# Bind to a specific IP address
./context-evaluator-linux-x64 api --host 51.159.136.32 --port 3001

# Development mode (localhost only, default)
./context-evaluator-linux-x64 api --host localhost --port 3001
```

**Why use 0.0.0.0?** This special address tells the server to listen on all available network interfaces, making it accessible via any IP address assigned to the machine (public IP, private IP, localhost).

### Option 2: Using Environment Variables

Environment variables provide an alternative configuration method:

```bash
# Using environment variables
HOST=0.0.0.0 PORT=3001 ./context-evaluator-linux-x64 api

# Or export them first
export HOST=0.0.0.0
export PORT=3001
./context-evaluator-linux-x64 api
```

**Priority:** CLI flags override environment variables, which override defaults.

### Option 3: Using npm Scripts (Development)

For development with source code:

```bash
# Public binding (accepts connections from any IP)
bun run start:public
```

### Verification

After starting the server with public binding, verify it's accessible:

```bash
# On the server (local test)
curl http://localhost:3001/api/health

# From another machine (replace with your server's IP)
curl http://51.159.136.32:3001/api/health
```

Expected response:
```json
{"status":"healthy","timestamp":"2026-01-28T..."}
```

### Security Considerations

**Development:**
- Keep default `localhost` binding for local development
- Prevents accidental network exposure during development
- No firewall configuration needed

**Production:**
- Use `--host 0.0.0.0` to accept connections on all interfaces
- Configure firewall rules to restrict access (e.g., allow only specific IPs)
- Consider using a reverse proxy (nginx, Caddy, Apache) for:
  - HTTPS/TLS termination
  - Rate limiting and DDoS protection
  - Load balancing across multiple instances
  - Better logging and monitoring

**Example nginx reverse proxy configuration:**

```nginx
server {
    listen 80;
    server_name yourdomain.com;

    location / {
        proxy_pass http://localhost:3001;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

**Firewall Configuration Example (Ubuntu/UFW):**

```bash
# Allow specific port
sudo ufw allow 3001/tcp

# Or allow only from specific IP
sudo ufw allow from 192.168.1.0/24 to any port 3001

# Check firewall status
sudo ufw status
```

### Common Issues

**Problem:** Server starts but external connections fail

**Solution:**
1. Verify server is bound to `0.0.0.0` or public IP (not `localhost`)
2. Check firewall allows traffic on the port (`sudo ufw status`)
3. Ensure cloud provider security groups allow inbound traffic
4. Verify no other service is using the port (`sudo lsof -i :3001`)

**Problem:** "Address already in use" error

**Solution:**
1. Check if another process is using the port: `sudo lsof -i :3001`
2. Stop the conflicting process or choose a different port
3. Kill the process if needed: `kill -9 <PID>`

---

# Frontend Color System

## Centralized Color Palette

All colors in the frontend MUST be centralized in `frontend/src/styles.css`. Do NOT add inline colors directly in components.

### Color Philosophy

The UI uses a **minimal, purposeful color palette**:
- **Primary accent**: Blue (`--color-primary`) - for interactive elements, buttons, links, focus states
- **Severity indicators**: Red/Orange/Yellow - ONLY for severity levels (Critical/High/Medium)
- **Neutral UI**: Slate variations - for backgrounds, borders, text, sections

### CSS Classes to Use

**Severity badges** (in `frontend/src/types/evaluation.ts`):
```ts
getSeverityColor(severity)  // Returns: 'severity-badge severity-critical' etc.
getSeverityBorderColor(severity)  // Returns RGB value for border colors
```

**Severity dots** (colored indicators):
```html
<div className="severity-dot severity-dot-critical" />
<div className="severity-dot severity-dot-high" />
<div className="severity-dot severity-dot-medium" />
<div className="severity-dot severity-dot-low" />
```

**Info sections** (unified neutral style):
```html
<div className="info-section">
  <div className="flex items-start gap-2">
    <svg className="info-section-icon">...</svg>
    <div>
      <p className="info-section-header">Section Title</p>
      <p className="info-section-content">Content here</p>
    </div>
  </div>
</div>
```

**Stat cards** (for summary metrics with severity tints):
```html
<div className="stat-card-critical">...</div>
<div className="stat-card-high">...</div>
<div className="stat-card-medium">...</div>
```

### Do NOT:
- Add `bg-purple-*`, `bg-green-*`, `bg-amber-*`, `bg-cyan-*`, etc. inline in components
- Use gradients like `bg-gradient-to-br from-X-500 to-Y-600` for decoration
- Use emoji as severity indicators (use `.severity-dot` classes instead)
- Define colors inline using Tailwind color classes without good reason

### Do:
- Use the predefined CSS classes from `styles.css`
- Keep the color palette limited to blue (primary) + slate (neutral) + severity colors
- Let icons and labels differentiate sections, not colors
- Add any new shared color classes to `styles.css` first

---

# Frontend Typography System

## Centralized Font Sizes

All font sizes in the frontend MUST use the centralized typography system in `frontend/src/styles.css`. Do NOT use arbitrary or hardcoded font sizes.

### Typography Scale (CSS Variables)

```css
--font-size-xs: 0.75rem;   /* 12px - captions, metadata */
--font-size-sm: 0.875rem;  /* 14px - body text, labels */
--font-size-base: 1rem;    /* 16px - standard content */
--font-size-lg: 1.125rem;  /* 18px - card headers */
--font-size-xl: 1.25rem;   /* 20px - section headers */
--font-size-2xl: 1.5rem;   /* 24px - stat numbers */
--font-size-3xl: 1.875rem; /* 30px - page titles */
```

### Semantic Typography Classes

Use these classes for consistent text styling across the app:

| Class | Use Case | Size |
|-------|----------|------|
| `.text-title` | Page titles | 3xl, bold |
| `.text-heading` | Section headers (h2) | xl, bold |
| `.text-subheading` | Card/panel headers (h3) | lg, bold |
| `.text-card-title` | Card titles, emphasized | base, semibold |
| `.text-body` | Primary body text | sm, slate-300 |
| `.text-body-muted` | Secondary body text | sm, slate-400 |
| `.text-caption` | Small text, metadata | xs, slate-400 |
| `.text-label` | Input labels | sm, semibold |
| `.text-label-upper` | Uppercase section labels | xs, uppercase, tracking |
| `.text-stat` | Stat/metric numbers | 2xl, bold |
| `.text-code` | Inline code, monospace | xs, mono |

### Component Classes with Typography

These component classes include proper font sizing:
- `.input-field` - includes `text-sm`
- `.btn-primary`, `.btn-secondary` - includes font sizing
- `.badge` - includes `text-xs`
- `.info-section-header` - includes `text-xs`
- `.info-section-content` - includes `text-sm`
- `.tab-button` - includes `text-sm`

### Do NOT:
- Use arbitrary sizes like `text-[14px]` or `text-[0.9rem]`
- Mix different size conventions within similar components
- Hardcode font sizes without using CSS variables or Tailwind classes

### Do:
- Use Tailwind's standard size classes (`text-xs`, `text-sm`, `text-base`, etc.)
- Prefer semantic typography classes for common patterns
- Ensure form inputs use consistent sizing (`.input-field` class)
- Add new typography patterns to `styles.css` first

<!-- rtk-instructions v2 -->
# RTK (Rust Token Killer) - Token-Optimized Commands

## Golden Rule

**Always prefix commands with `rtk`**. If RTK has a dedicated filter, it uses it. If not, it passes through unchanged. This means RTK is always safe to use.

**Important**: Even in command chains with `&&`, use `rtk`:
```bash
# ❌ Wrong
git add . && git commit -m "msg" && git push

# ✅ Correct
rtk git add . && rtk git commit -m "msg" && rtk git push
```

## RTK Commands by Workflow

### Build & Compile (80-90% savings)
```bash
rtk cargo build         # Cargo build output
rtk cargo check         # Cargo check output
rtk cargo clippy        # Clippy warnings grouped by file (80%)
rtk tsc                 # TypeScript errors grouped by file/code (83%)
rtk lint                # ESLint/Biome violations grouped (84%)
rtk prettier --check    # Files needing format only (70%)
rtk next build          # Next.js build with route metrics (87%)
```

### Test (90-99% savings)
```bash
rtk cargo test          # Cargo test failures only (90%)
rtk vitest run          # Vitest failures only (99.5%)
rtk playwright test     # Playwright failures only (94%)
rtk test <cmd>          # Generic test wrapper - failures only
```

### Git (59-80% savings)
```bash
rtk git status          # Compact status
rtk git log             # Compact log (works with all git flags)
rtk git diff            # Compact diff (80%)
rtk git show            # Compact show (80%)
rtk git add             # Ultra-compact confirmations (59%)
rtk git commit          # Ultra-compact confirmations (59%)
rtk git push            # Ultra-compact confirmations
rtk git pull            # Ultra-compact confirmations
rtk git branch          # Compact branch list
rtk git fetch           # Compact fetch
rtk git stash           # Compact stash
rtk git worktree        # Compact worktree
```

Note: Git passthrough works for ALL subcommands, even those not explicitly listed.

### GitHub (26-87% savings)
```bash
rtk gh pr view <num>    # Compact PR view (87%)
rtk gh pr checks        # Compact PR checks (79%)
rtk gh run list         # Compact workflow runs (82%)
rtk gh issue list       # Compact issue list (80%)
rtk gh api              # Compact API responses (26%)
```

### JavaScript/TypeScript Tooling (70-90% savings)
```bash
rtk pnpm list           # Compact dependency tree (70%)
rtk pnpm outdated       # Compact outdated packages (80%)
rtk pnpm install        # Compact install output (90%)
rtk npm run <script>    # Compact npm script output
rtk npx <cmd>           # Compact npx command output
rtk prisma              # Prisma without ASCII art (88%)
```

### Files & Search (60-75% savings)
```bash
rtk ls <path>           # Tree format, compact (65%)
rtk read <file>         # Code reading with filtering (60%)
rtk grep <pattern>      # Search grouped by file (75%)
rtk find <pattern>      # Find grouped by directory (70%)
```

### Analysis & Debug (70-90% savings)
```bash
rtk err <cmd>           # Filter errors only from any command
rtk log <file>          # Deduplicated logs with counts
rtk json <file>         # JSON structure without values
rtk deps                # Dependency overview
rtk env                 # Environment variables compact
rtk summary <cmd>       # Smart summary of command output
rtk diff                # Ultra-compact diffs
```

### Infrastructure (85% savings)
```bash
rtk docker ps           # Compact container list
rtk docker images       # Compact image list
rtk docker logs <c>     # Deduplicated logs
rtk kubectl get         # Compact resource list
rtk kubectl logs        # Deduplicated pod logs
```

### Network (65-70% savings)
```bash
rtk curl <url>          # Compact HTTP responses (70%)
rtk wget <url>          # Compact download output (65%)
```

### Meta Commands
```bash
rtk gain                # View token savings statistics
rtk gain --history      # View command history with savings
rtk discover            # Analyze Claude Code sessions for missed RTK usage
rtk proxy <cmd>         # Run command without filtering (for debugging)
rtk init                # Add RTK instructions to CLAUDE.md
rtk init --global       # Add RTK to ~/.claude/CLAUDE.md
```

## Token Savings Overview

| Category | Commands | Typical Savings |
|----------|----------|-----------------|
| Tests | vitest, playwright, cargo test | 90-99% |
| Build | next, tsc, lint, prettier | 70-87% |
| Git | status, log, diff, add, commit | 59-80% |
| GitHub | gh pr, gh run, gh issue | 26-87% |
| Package Managers | pnpm, npm, npx | 70-90% |
| Files | ls, read, grep, find | 60-75% |
| Infrastructure | docker, kubectl | 85% |
| Network | curl, wget | 65-70% |

Overall average: **60-90% token reduction** on common development operations.
<!-- /rtk-instructions -->